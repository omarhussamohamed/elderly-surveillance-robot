{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bea9334c",
   "metadata": {},
   "source": [
    "# Experiment Notebook\n",
    "\n",
    "This notebook documents **experimental training and evaluation work** conducted during the development of the **Elderly Monitoring AI System**.\n",
    "\n",
    "⚠️ **Important**  \n",
    "- This notebook is for **research and experimentation only**.  \n",
    "- It is **NOT part of the runtime system**.  \n",
    "- The production inference code is located in `/src/layers/fire_detection`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06a8dce",
   "metadata": {},
   "source": [
    "# Fire, Smoke, and Person Detection (YOLOv8)\n",
    "\n",
    "This experiment trains and evaluates a **YOLOv8-based object detection model** capable of detecting:\n",
    "- Fire\n",
    "- Smoke\n",
    "- Person\n",
    "\n",
    "The model produced here is used **only for inference** in the final system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012239e4",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "- Merge multiple public datasets into a unified detection dataset\n",
    "- Train a multi-class YOLOv8 model\n",
    "- Evaluate detection performance on train, validation, and test splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c4a531",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "- Python 3.10\n",
    "- Ultralytics YOLOv8\n",
    "- OpenCV\n",
    "- NumPy\n",
    "- KaggleHub\n",
    "\n",
    "This notebook was designed for execution in **Google Colab**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc114878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dependencies installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q ultralytics kagglehub\n",
    "print(\"✅ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f0be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import requests\n",
    "from collections import Counter\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "import kagglehub\n",
    "\n",
    "print(\"✅ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e4064",
   "metadata": {},
   "source": [
    "## Dataset Download\n",
    "\n",
    "This experiment uses:\n",
    "- A public **person detection dataset** (Kaggle)\n",
    "- An **indoor fire & smoke dataset** (Zenodo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1e1a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_datasets():\n",
    "    if not os.path.exists('/content/person'):\n",
    "        path = kagglehub.dataset_download(\"samuelayman/person\")\n",
    "        shutil.copytree(path, \"/content/person\", dirs_exist_ok=True)\n",
    "        print(\"Person dataset downloaded\")\n",
    "    else:\n",
    "        print(\"Person dataset exists\")\n",
    "\n",
    "    if not os.path.exists('/content/smoke_fire'):\n",
    "        url = \"https://zenodo.org/records/15826133/files/Indoor%20Fire%20Smoke.zip\"\n",
    "        r = requests.get(url)\n",
    "        ZipFile(BytesIO(r.content)).extractall('/content/smoke_fire')\n",
    "        print(\"Fire/Smoke dataset downloaded\")\n",
    "    else:\n",
    "        print(\"Fire/Smoke dataset exists\")\n",
    "\n",
    "download_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eff76c0",
   "metadata": {},
   "source": [
    "## Dataset Inspection\n",
    "\n",
    "Basic checks are performed to verify dataset structure and file counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126834b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"=== Person Dataset ===\"\n",
    "!find /content/person -type f | head -10\n",
    "\n",
    "!echo \"\\n=== Fire/Smoke Dataset ===\"\n",
    "!find /content/smoke_fire -type f | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c1aa9f",
   "metadata": {},
   "source": [
    "## Dataset Merging and Class Mapping\n",
    "\n",
    "The datasets are merged into a single YOLO-compatible structure.\n",
    "\n",
    "Final class mapping:\n",
    "- `0` → fire\n",
    "- `1` → person\n",
    "- `2` → smoke\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95aa474",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('merged_dataset'):\n",
    "    shutil.rmtree('merged_dataset')\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    os.makedirs(f'merged_dataset/{split}/images', exist_ok=True)\n",
    "    os.makedirs(f'merged_dataset/{split}/labels', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4830779",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_img_subdirs = ['Train', 'Test', 'Validate']\n",
    "person_base_img = '/content/person/person/Images'\n",
    "person_base_lbl = '/content/person/person/Labels'\n",
    "\n",
    "person_img_files = []\n",
    "person_label_paths = {}\n",
    "\n",
    "for sub in person_img_subdirs:\n",
    "    img_dir = os.path.join(person_base_img, sub)\n",
    "    lbl_dir = os.path.join(person_base_lbl, sub)\n",
    "\n",
    "    if os.path.exists(img_dir):\n",
    "        for f in os.listdir(img_dir):\n",
    "            if f.lower().endswith(('.jpg', '.png')):\n",
    "                person_img_files.append(os.path.join(img_dir, f))\n",
    "\n",
    "    if os.path.exists(lbl_dir):\n",
    "        for f in os.listdir(lbl_dir):\n",
    "            if f.endswith('.txt'):\n",
    "                person_label_paths[os.path.splitext(f)[0]] = os.path.join(lbl_dir, f)\n",
    "\n",
    "fs_img_dir = '/content/smoke_fire/Indoor Fire Smoke/train/images'\n",
    "fs_lbl_dir = '/content/smoke_fire/Indoor Fire Smoke/train/labels'\n",
    "fs_img_files = [os.path.join(fs_img_dir, f) for f in os.listdir(fs_img_dir) if f.lower().endswith(('.jpg', '.png'))]\n",
    "\n",
    "all_images = person_img_files + fs_img_files\n",
    "random.shuffle(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d3dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(all_images)\n",
    "train_end = int(0.7 * n)\n",
    "val_end = int(0.9 * n)\n",
    "\n",
    "splits = {\n",
    "    'train': all_images[:train_end],\n",
    "    'val': all_images[train_end:val_end],\n",
    "    'test': all_images[val_end:]\n",
    "}\n",
    "\n",
    "counts = Counter()\n",
    "for split, paths in splits.items():\n",
    "    for img_path in paths:\n",
    "        img_name = os.path.basename(img_path)\n",
    "        shutil.copy(img_path, f'merged_dataset/{split}/images/{img_name}')\n",
    "\n",
    "        base = os.path.splitext(img_name)[0]\n",
    "        lbl_out = f'merged_dataset/{split}/labels/{base}.txt'\n",
    "\n",
    "        if 'person' in img_path:\n",
    "            src_lbl = person_label_paths.get(base)\n",
    "        else:\n",
    "            src_lbl = os.path.join(fs_lbl_dir, base + '.txt')\n",
    "\n",
    "        if src_lbl and os.path.exists(src_lbl):\n",
    "            with open(src_lbl) as f:\n",
    "                lines = f.readlines()\n",
    "            with open(lbl_out, 'w') as f:\n",
    "                for line in lines:\n",
    "                    parts = line.split()\n",
    "                    old_class = int(parts[0])\n",
    "                    new_class = 1 if 'person' in img_path else (0 if old_class == 0 else 2)\n",
    "                    f.write(f\"{new_class} {' '.join(parts[1:])}\\n\")\n",
    "                    counts[new_class] += 1\n",
    "        else:\n",
    "            open(lbl_out, 'w').close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1db122",
   "metadata": {},
   "source": [
    "## Dataset Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3cd3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, name in zip([0,1,2], ['fire','person','smoke']):\n",
    "    print(f\"Class {c} ({name}): {counts[c]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d0a173",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_yaml = \"\"\"\n",
    "path: /content/merged_dataset\n",
    "train: train/images\n",
    "val: val/images\n",
    "test: test/images\n",
    "\n",
    "nc: 3\n",
    "names: ['fire', 'person', 'smoke']\n",
    "\"\"\"\n",
    "\n",
    "with open('data.yaml', 'w') as f:\n",
    "    f.write(data_yaml.strip())\n",
    "\n",
    "print(\"data.yaml created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534a0be1",
   "metadata": {},
   "source": [
    "## Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b854b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8s.pt')\n",
    "\n",
    "results = model.train(\n",
    "    data='data.yaml',\n",
    "    epochs=100,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    augment=True,\n",
    "    patience=20,\n",
    "    device=0,\n",
    "    optimizer='AdamW',\n",
    "    lr0=0.001,\n",
    "    cos_lr=True,\n",
    "    mixup=0.1,\n",
    "    copy_paste=0.1\n",
    ")\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a09bd0",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54406ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('runs/detect/train/weights/best.pt')\n",
    "val_results = model.val(data='data.yaml', split='val')\n",
    "test_results = model.val(data='data.yaml', split='test', plots=True)\n",
    "\n",
    "print(f\"Validation mAP@50-95: {val_results.box.map:.3f}\")\n",
    "print(f\"Test mAP@50-95: {test_results.box.map:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237e781f",
   "metadata": {},
   "source": [
    "## Observations & Notes\n",
    "\n",
    "- Dataset merging was a critical step for multi-class consistency.\n",
    "- Augmentation improved smoke detection robustness.\n",
    "- Final model balances accuracy and real-time performance.\n",
    "- The trained weights are used **only for inference** in the final system.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
